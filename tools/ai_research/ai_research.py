#!/usr/bin/env python3
"""
Multi-Source AI Research Tool
Daily briefing generator: GitHub Trending + arXiv papers.

Usage:
    python3 ai_research.py
    python3 ai_research.py --keywords "MCP agent tool-use" --since weekly --output report.md
"""

import argparse
import urllib.request
import urllib.parse
import re
import html
from datetime import datetime, timezone
from pathlib import Path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Fetchers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch(url, accept=None):
    headers = {"User-Agent": "ai-research-tool/1.0 (github.com/kanosa0101/openclaw-system)"}
    if accept:
        headers["Accept"] = accept
    req = urllib.request.Request(url, headers=headers)
    for attempt in range(3):
        try:
            with urllib.request.urlopen(req, timeout=15) as r:
                return r.read().decode(errors="replace")
        except Exception as e:
            if attempt == 2:
                return ""
    return ""


def github_trending(since="daily", language=""):
    """Return list of {repo, url, description, stars_today}."""
    url = f"https://github.com/trending{('/' + language) if language else ''}?since={since}"
    raw = fetch(url)
    results = []
    # repo slugs
    slugs = re.findall(r'href="/([A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+)"', raw)
    seen = set()
    clean = []
    for s in slugs:
        if s not in seen and "/" in s and not any(x in s for x in ["trending", "login", "signup", "explore"]):
            seen.add(s)
            clean.append(s)

    stars = re.findall(r'([\d,]+)\s+stars? today', raw)
    descs = re.findall(r'<p class="col-9[^"]*">\s*(.*?)\s*</p>', raw, re.S)

    for i, slug in enumerate(clean[:15]):
        results.append({
            "repo": slug,
            "url": f"https://github.com/{slug}",
            "description": html.unescape(descs[i].strip()) if i < len(descs) else "",
            "stars_today": stars[i].replace(",", "") if i < len(stars) else "?",
        })
    return results


def arxiv_papers(query, max_results=10):
    """Return list of {title, url, published, summary, authors}."""
    q = urllib.parse.quote(query)
    url = (f"https://export.arxiv.org/api/query?"
           f"search_query=all:{q}&sortBy=submittedDate&sortOrder=descending&max_results={max_results}")
    raw = fetch(url)
    papers = []
    for entry in re.findall(r"<entry>(.*?)</entry>", raw, re.S):
        def tag(t):
            m = re.search(rf"<{t}[^>]*>(.*?)</{t}>", entry, re.S)
            return html.unescape(m.group(1).strip()) if m else ""

        title = tag("title").replace("\n", " ")
        summary = tag("summary").replace("\n", " ")[:350]
        published = tag("published")[:10]
        link_m = re.search(r"<id>(http[s]?://arxiv\.org/abs/[^<]+)</id>", entry)
        url_paper = link_m.group(1).strip() if link_m else ""
        authors = re.findall(r"<name>(.*?)</name>", entry)

        if title and url_paper:
            papers.append({
                "title": title,
                "url": url_paper,
                "published": published,
                "summary": summary,
                "authors": authors[:3],
            })
    return papers


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Report generator
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate(repos, papers, keywords, since, top_n=5):
    date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    now = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
    since_label = {"daily": "ä»Šæ—¥", "weekly": "æœ¬å‘¨", "monthly": "æœ¬æœˆ"}.get(since, since)

    lines = [
        f"# æ¯æ—¥ AI è°ƒç ”ç®€æŠ¥ â€” {date}",
        "",
        f"> ç”Ÿæˆæ—¶é—´ï¼š{now} | å…³é”®è¯ï¼š`{keywords}` | æ•°æ®æ¥æºï¼šGitHub Trending Â· arXiv",
        "",
        "---",
        "",
        f"## ğŸ”¥ GitHub {since_label}çƒ­é—¨ï¼ˆTop {min(top_n, len(repos))}ï¼‰",
        "",
    ]

    for r in repos[:top_n]:
        stars = f"â­ +{r['stars_today']}" if r["stars_today"] != "?" else ""
        desc = f" â€” {r['description']}" if r["description"] else ""
        lines.append(f"- **[{r['repo']}]({r['url']})** {stars}{desc}")

    lines += [
        "",
        f"## ğŸ“š arXiv æœ€æ–°è®ºæ–‡ï¼ˆTop {min(top_n, len(papers))}ï¼‰",
        "",
    ]

    for p in papers[:top_n]:
        authors = ", ".join(p["authors"]) + (" et al." if len(p["authors"]) >= 3 else "")
        lines.append(f"### [{p['title']}]({p['url']})")
        lines.append(f"*{p['published']} Â· {authors}*")
        lines.append("")
        lines.append(f"{p['summary']}...")
        lines.append("")

    lines += [
        "---",
        "",
        "## ğŸ’¡ ä»Šæ—¥æ´å¯Ÿ",
        "",
        "*(ç”±ä½¿ç”¨è€…æˆ– AI è¡¥å……)*",
        "",
        "---",
        f"*è‡ªåŠ¨ç”Ÿæˆ by [openclaw-system](https://github.com/kanosa0101/openclaw-system)*",
    ]

    return "\n".join(lines)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(description="Multi-Source AI Research Tool")
    parser.add_argument("--keywords", default="LLM agent agentic self-reflection",
                        help="arXiv æœç´¢å…³é”®è¯ï¼ˆé»˜è®¤ï¼šLLM agent agenticï¼‰")
    parser.add_argument("--since", default="daily",
                        choices=["daily", "weekly", "monthly"],
                        help="GitHub Trending æ—¶é—´èŒƒå›´ï¼ˆé»˜è®¤ï¼šdailyï¼‰")
    parser.add_argument("--language", default="",
                        help="GitHub Trending è¯­è¨€è¿‡æ»¤ï¼ˆå¦‚ pythonï¼‰")
    parser.add_argument("--top", type=int, default=5,
                        help="æ¯ä¸ªæ¥æºæ˜¾ç¤ºçš„æ¡ç›®æ•°ï¼ˆé»˜è®¤ï¼š5ï¼‰")
    parser.add_argument("--output", default=None,
                        help="è¾“å‡º Markdown æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤ï¼šæ‰“å°åˆ°ç»ˆç«¯ï¼‰")
    args = parser.parse_args()

    print("ğŸ“¡ æŠ“å– GitHub Trending...", flush=True)
    repos = github_trending(args.since, args.language)
    print(f"   æ‰¾åˆ° {len(repos)} ä¸ªé¡¹ç›®")

    print("ğŸ“¡ æŠ“å– arXiv è®ºæ–‡...", flush=True)
    papers = arxiv_papers(args.keywords, max_results=args.top * 2)
    print(f"   æ‰¾åˆ° {len(papers)} ç¯‡è®ºæ–‡")

    report = generate(repos, papers, args.keywords, args.since, args.top)

    if args.output:
        out = Path(args.output)
        out.parent.mkdir(parents=True, exist_ok=True)
        out.write_text(report, encoding="utf-8")
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜ï¼š{args.output}")
    else:
        print("\n" + report)


if __name__ == "__main__":
    main()
